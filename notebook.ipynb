{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for comet\n",
    "#from comet_ml import Experiment\n",
    "\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from dataset import ptb\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cpu'\n",
    "device = 'gpu'\n",
    "\n",
    "np = numpy\n",
    "if device == 'gpu':\n",
    "    import cupy\n",
    "    import cupyx\n",
    "    np = cupy\n",
    "print(f'Use {device}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x, ndim=-1):\n",
    "    x = x.astype('float64')\n",
    "    if x.ndim == -1:\n",
    "        ndim = len(x.shape) - 1\n",
    "    c = x.max()\n",
    "    exp_x = np.exp(x - c)\n",
    "    sum_exp_x = np.sum(exp_x, axis=ndim)\n",
    "    out = (exp_x.T / sum_exp_x).T\n",
    "    return out.astype('f')\n",
    "\n",
    "def cross_entropy_error(y, t, onehot=False):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    if not onehot:\n",
    "        out = -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "    else:\n",
    "        out = -np.sum(np.dot(t, np.log(y + 1e-7))) / batch_size\n",
    "    return out\n",
    "\n",
    "def numerical_diff(f, x, i):\n",
    "    h = 1e-4\n",
    "    h_vec = np.zeros_like(x)\n",
    "    h_vec[i] = h\n",
    "    return (f(x + h_vec) - f(x - h_vec)) / (2*h)\n",
    "\n",
    "def numerical_diff2(f, x, i, j):\n",
    "    h = 1e-4\n",
    "    h_vec = np.zeros_like(x)\n",
    "    h_vec[i, j] = h\n",
    "    return (f(x + h_vec) - f(x - h_vec)) / (2*h)\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    grad = np.zeros_like(x).astype(np.float128)\n",
    "    n, m = x.shape\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            grad[i, j] = numerical_diff2(f, x, i, j)\n",
    "    return grad\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "def to_cpu(x):\n",
    "    if type(x) == numpy.ndarray:\n",
    "        return x\n",
    "    return cupy.asnumpy(x)\n",
    "\n",
    "def to_gpu(x):\n",
    "    if type(x) == cupy.ndarray:\n",
    "        return x\n",
    "    return cupy.array(x)\n",
    "\n",
    "def to_device(x, device=device):\n",
    "    if device == 'gpu':\n",
    "        return to_gpu(x)\n",
    "    else:\n",
    "        return to_cpu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, shape, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=10**(-8)):\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = np.zeros(shape)\n",
    "        self.v = np.zeros(shape)\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, w, dw):\n",
    "        self.t += 1\n",
    "        self.m = (self.beta1 * self.m) + (1 - self.beta1) * dw\n",
    "        self.v = (self.beta2 * self.v) + (1 - self.beta2) * dw**2\n",
    "        mh = self.m / (1 - self.beta1 ** self.t)\n",
    "        vh = self.v / (1 - self.beta2 ** self.t)\n",
    "        w -= self.alpha * (mh / (np.sqrt(vh) + self.epsilon))\n",
    "\n",
    "class AdamContainer:\n",
    "    def __init__(self, layers, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=10**(-8)):\n",
    "        self.params = []\n",
    "        for params in [layer.params for layer in layers]:\n",
    "            for param in params:\n",
    "                self.params.append(param)\n",
    "        self.grads = []\n",
    "        for grads in [layer.grads for layer in layers]:\n",
    "            for grad in grads:\n",
    "                self.grads.append(grad)\n",
    "        self.adams = [Adam(param.shape, alpha, beta1, beta2, epsilon) for param in self.params]\n",
    "    \n",
    "    def update(self):\n",
    "        for adam, param, grad in zip(self.adams, self.params, self.grads):\n",
    "            adam.update(param, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BaseLayer:\n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "        self.grads = None\n",
    "        \n",
    "    def to_cpu(self):\n",
    "        for param in self.params:\n",
    "            param = to_cpu(param)\n",
    "        for grad in self.grads:\n",
    "            grad = to_cpu(grad)\n",
    "            \n",
    "    def to_gpu(self):\n",
    "        for param in self.params:\n",
    "            param = to_gpu(param)\n",
    "        for grad in self.grads:\n",
    "            grad = to_gpu(grad)\n",
    "\n",
    "class Affine(BaseLayer):\n",
    "    def __init__(self, w, b):\n",
    "        self.params = [w, b]\n",
    "        self.grads = [np.zeros_like(w), np.zeros_like(b)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        w, b = self.params\n",
    "        self.x = x\n",
    "        return np.dot(x, w) + b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        w = self.params[0]\n",
    "        dx = np.dot(dout, w.T)\n",
    "        self.grads[0] = self.dw = np.dot(self.x.T, dout)\n",
    "        self.grads[1] = self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "\n",
    "class TimeAffine(BaseLayer):\n",
    "    def __init__(self, w, b):\n",
    "        self.params = [w, b]\n",
    "        self.grads = [np.zeros_like(w), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        w, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, V = w.shape\n",
    "        \n",
    "        out = np.empty((N, T, V), dtype='f')\n",
    "        self.layers = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Affine(w, b)\n",
    "            out[:,t,:] = layer.forward(xs[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        self.cache = (N, T, D, V)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, D, V = self.cache\n",
    "        \n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        dw = np.empty((D, T, V), dtype='f')\n",
    "        db = np.empty((T, V), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            out[:,t,:] = layer.backward(dout[:,t,:])\n",
    "            dw[:,t,:] = layer.dw\n",
    "            db[t,:] = layer.db\n",
    "        self.grads[0] = self.dw = dw.sum(axis=1)\n",
    "        self.grads[1] = self.db = db.sum(axis=0)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ReLU(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        return dout\n",
    "\n",
    "class Sigmoid(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = sigmoid(x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx\n",
    "    \n",
    "class Softmax(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.y = softmax(x)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = self.y * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.y * sumdx\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1, onehot=False):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if not onehot:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx *= dout\n",
    "            dx = dx / batch_size\n",
    "        else:\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        return dx\n",
    "\n",
    "class TimeSoftmaxWithLoss(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.layers = None\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        \n",
    "        # if ts is one-hot vector\n",
    "        if ts.ndim == 3:\n",
    "            ts = ts.argmax(axis=2).reshape(N, T)\n",
    "        \n",
    "        ys = np.empty(T, dtype='f')\n",
    "        self.layers = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = SoftmaxWithLoss()\n",
    "            ys[t] = layer.forward(xs[:,t,:], ts[:,t])\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        loss = ys.sum() / T\n",
    "        \n",
    "        self.cache = (N, T, V)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        N, T, V = self.cache\n",
    "        dx = np.empty((N, T, V), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dx[:,t,:] = layer.backward()\n",
    "        self.dx = dx / T\n",
    "        \n",
    "        return self.dx\n",
    "\n",
    "class Dropout(BaseLayer):\n",
    "    def __init__(self, ratio=0.5):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.ratio = ratio\n",
    "        self.mask = None\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.train:\n",
    "            self.mask = np.random.rand(*x.shape) > self.ratio\n",
    "            self.mask = self.mask.astype('f') / (1.0 - self.ratio)\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "class Embedding(BaseLayer):\n",
    "    def __init__(self, w):\n",
    "        self.params = [w]\n",
    "        self.grads = [np.zeros_like(w)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        w = self.params[0]\n",
    "        self.idx = idx\n",
    "        return w[idx]\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        if device == 'gpu':\n",
    "            add_at = cupyx.scatter_add\n",
    "        else:\n",
    "            add_at = np.add.at\n",
    "            \n",
    "        dw = self.grads[0]\n",
    "        dw[...] = 0\n",
    "        add_at(dw, self.idx, dout)\n",
    "        self.grads[0] = self.dw = dw\n",
    "        return None\n",
    "\n",
    "class TimeEmbedding(BaseLayer):\n",
    "    def __init__(self, w):\n",
    "        self.params = [w]\n",
    "        self.grads = [np.zeros_like(w)]\n",
    "        self.layers = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        w = self.params[0]\n",
    "        N, T = xs.shape\n",
    "        V, D = w.shape\n",
    "        \n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Embedding(w)\n",
    "            out[:,t,:] = layer.forward(xs[:,t])\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "        \n",
    "        dw = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:,t,:])\n",
    "            dw += layer.dw\n",
    "        self.grads[0] = self.dw = dw\n",
    "        \n",
    "        return None\n",
    "\n",
    "class RNN(BaseLayer):\n",
    "    def __init__(self, wx, wh, b):\n",
    "        self.params = [wx, wh, b]\n",
    "        self.grads = [np.zeros_like(wx), np.zeros_like(wh), np.zeros_like(b)]\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        wx, wh, b = self.params\n",
    "        t = np.dot(h_prev, wh) + np.dot(x, wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        wx, wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next * (1 - dh_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dwh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, wh.T)\n",
    "        dwx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, wx.T)\n",
    "        self.grads[0][...] = self.dwx = dwx\n",
    "        self.grads[1][...] = self.dwh = dwh\n",
    "        self.grads[2][...] = self.db = db\n",
    "        \n",
    "        return dx, dh_prev\n",
    "\n",
    "class TimeRNN(BaseLayer):\n",
    "    def __init__(self, wx, wh, b, stateful=False):\n",
    "        self.params = [wx, wh, b]\n",
    "        self.grads = [np.zeros_like(wx), np.zeros_like(wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "        \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        wx, wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = wx.shape\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:,t,:], self.h)\n",
    "            hs[:,t,:] = self.h\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        wx, wh, b  = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = wx.shape\n",
    "        \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:,t,:] + dh)\n",
    "            dxs[:,t,:] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "        \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dwx = self.grads[0]\n",
    "        self.dwh = self.grads[1]\n",
    "        self.db = self.grads[2]\n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs\n",
    "\n",
    "class LSTM(BaseLayer):\n",
    "    def __init__(self, wx, wh, b):\n",
    "        self.params = [wx, wh, b]\n",
    "        self.grads = [np.zeros_like(wx), np.zeros_like(wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        wx, wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "        \n",
    "        A = np.dot(h_prev, wh) + np.dot(x, wx) + b\n",
    "        \n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "        \n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "        \n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "        \n",
    "        self.cache = (x, h_prev, c_prev,i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        wx, wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "        \n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "        \n",
    "        dc_prev = ds * f\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "        \n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= 1 - g ** 2\n",
    "        \n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "        \n",
    "        dwh = np.dot(h_prev.T, dA)\n",
    "        dh_prev = np.dot(dA, wh.T)\n",
    "        dwx = np.dot(x.T, dA)\n",
    "        dx = np.dot(dA, wx.T)\n",
    "        db = dA.sum(axis=0)\n",
    "        \n",
    "        self.grads[0][...] = self.dwx = dwx\n",
    "        self.grads[1][...] = self.dwh = dwh\n",
    "        self.grads[2][...] = self.db = db\n",
    "        \n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "class TimeLSTM(BaseLayer):\n",
    "    def __init__(self, wx, wh, b, stateful=False):\n",
    "        self.params = [wx, wh, b]\n",
    "        self.grads = [np.zeros_like(wx), np.zeros_like(wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        self.h, self.dh, self.c = None, None, None\n",
    "        self.stateful = stateful\n",
    "        \n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        wx, wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = wh.shape[0]\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:,t,:], self.h, self.c)\n",
    "            hs[:,t,:] = self.h\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        wx, wh, b  = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = wx.shape[0]\n",
    "        \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:,t,:] + dh, dc)\n",
    "            dxs[:,t,:] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "        \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dwx = self.grads[0]\n",
    "        self.dwh = self.grads[1]\n",
    "        self.db = self.grads[2]\n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs\n",
    "\n",
    "class BaseNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = None\n",
    "        self.lastLayer = None\n",
    "        self.adam = None\n",
    "        \n",
    "    def train(self, x, t):\n",
    "        # forward\n",
    "        loss = self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.lastLayer.backward()\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        self.update()\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def update(self):\n",
    "        for grads in [layer.grads for layer in self.layers.values()]:\n",
    "            clip_grads(grads, self.max_grad)\n",
    "        self.adam.update()\n",
    "    \n",
    "    def save(self, state_file_name = 'network.state.pkl'):\n",
    "        params = self.adam.params.copy()\n",
    "        for i, param in enumerate(params):\n",
    "            params[i] = to_cpu(params[i])\n",
    "            \n",
    "        with open(state_file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "            print(f'Saved: {state_file_name}')\n",
    "            \n",
    "    def load(self, state_file_name = 'network.state.pkl'):\n",
    "        if os.path.exists(state_file_name):\n",
    "            with open(state_file_name, 'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "                for i, param in enumerate(params):\n",
    "                    self.adam.params[i][:] = to_device(param, device)\n",
    "                print(f'Loaded: {state_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(BaseNetwork):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size, max_grad=5.0, dropout_ratio=0.1):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        self.max_grad = max_grad\n",
    "        \n",
    "        embed_w = (rn(V, D) / 100).astype('f')\n",
    "        lstm1_wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm1_wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm1_b = np.zeros(4 * H).astype('f')\n",
    "        lstm2_wx = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm2_wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm2_b = np.zeros(4 * H).astype('f')\n",
    "        ## if use weight tying (require: H == D), comment-out this affine_w\n",
    "        affine_w = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Embedding'] = TimeEmbedding(embed_w)\n",
    "        self.layers['Dropout1'] = Dropout(dropout_ratio)\n",
    "        self.layers['LSTM1'] = TimeLSTM(lstm1_wx, lstm1_wh, lstm1_b, stateful=True)\n",
    "        self.layers['Dropout2'] = Dropout(dropout_ratio)\n",
    "        self.layers['LSTM2'] = TimeLSTM(lstm2_wx, lstm2_wh, lstm2_b, stateful=True)\n",
    "        self.layers['Dropout3'] = Dropout(dropout_ratio)\n",
    "        self.layers['Affine'] = TimeAffine(affine_w, affine_b)\n",
    "        #self.layers['Affine'] = TimeAffine(embed_w.T, affine_b)  # weight tying\n",
    "        self.lastLayer = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.lstm_layers = [self.layers['LSTM1'], self.layers['LSTM2']]\n",
    "        self.drop_layers = [self.layers['Dropout1'], self.layers['Dropout2'], self.layers['Dropout3']]\n",
    "        \n",
    "        self.adam = AdamContainer(list(self.layers.values()))\n",
    "        \n",
    "    def predict(self, x, train=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train = train\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t, train=True):\n",
    "        y = self.predict(x, train)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def reset_rnn_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(BaseLayer):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_w = (rn(V, D) / 100).astype('f')\n",
    "        lstm_wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Embedding'] = TimeEmbedding(embed_w)\n",
    "        self.layers['LSTM'] = TimeLSTM(lstm_wx, lstm_wh, lstm_b, stateful=False)\n",
    "        \n",
    "        self.params = []\n",
    "        for params in [layer.params for layer in self.layers.values()]:\n",
    "            for param in params:\n",
    "                self.params.append(param)\n",
    "        self.grads = []\n",
    "        for grads in [layer.grads for layer in self.layers.values()]:\n",
    "            for grad in grads:\n",
    "                self.grads.append(grad)\n",
    "        self.hs = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        for layer in self.layers.values():\n",
    "            xs = layer.forward(xs)\n",
    "        self.hs = xs\n",
    "        return xs[:,-1,:]\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:,-1,:] = dh\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dhs = layer.backward(dhs)\n",
    "        return dhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(BaseLayer):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_w = (rn(V, D) / 100).astype('f')\n",
    "        lstm_wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        affine_w = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Embedding'] = TimeEmbedding(embed_w)\n",
    "        self.layers['LSTM'] = TimeLSTM(lstm_wx, lstm_wh, lstm_b, stateful=True)\n",
    "        self.layers['Affine'] = TimeAffine(affine_w, affine_b)\n",
    "        \n",
    "        self.params = []\n",
    "        for params in [layer.params for layer in self.layers.values()]:\n",
    "            for param in params:\n",
    "                self.params.append(param)\n",
    "        self.grads = []\n",
    "        for grads in [layer.grads for layer in self.layers.values()]:\n",
    "            for grad in grads:\n",
    "                self.grads.append(grad)\n",
    "        \n",
    "    def forward(self, xs, h):\n",
    "        self.layers['LSTM'].set_state(h)\n",
    "        for layer in self.layers.values():\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dh = layer.backward(dh)\n",
    "        return self.layers['LSTM'].dh\n",
    "    \n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.layers['LSTM'].set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape(1,1)\n",
    "            for layer in self.layers.values():\n",
    "                x = layer.forward(x)\n",
    "            sample_id = np.argmax(x.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "\n",
    "        return np.array(sampled)\n",
    "\n",
    "class PeekyDecoder(BaseLayer):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_w = (rn(V, D) / 100).astype('f')\n",
    "        lstm_wx = (rn(H + D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        affine_w = (rn(H + H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Embedding'] = TimeEmbedding(embed_w)\n",
    "        self.layers['LSTM'] = TimeLSTM(lstm_wx, lstm_wh, lstm_b, stateful=True)\n",
    "        self.layers['Affine'] = TimeAffine(affine_w, affine_b)\n",
    "        \n",
    "        self.params = []\n",
    "        for params in [layer.params for layer in self.layers.values()]:\n",
    "            for param in params:\n",
    "                self.params.append(param)\n",
    "        self.grads = []\n",
    "        for grads in [layer.grads for layer in self.layers.values()]:\n",
    "            for grad in grads:\n",
    "                self.grads.append(grad)\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, xs, h):\n",
    "        N, T = xs.shape\n",
    "        N, H = h.shape\n",
    "        \n",
    "        self.layers['LSTM'].set_state(h)\n",
    "        \n",
    "        out = self.layers['Embedding'].forward(xs)\n",
    "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "        \n",
    "        out = self.layers['LSTM'].forward(out)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "        \n",
    "        score = self.layers['Affine'].forward(out)\n",
    "        self.cache = H\n",
    "        return score\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        H = self.cache\n",
    "        \n",
    "        dout = self.layers['Affine'].backward(dh)\n",
    "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
    "        \n",
    "        dout = self.layers['LSTM'].backward(dout)\n",
    "        dout, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
    "        \n",
    "        self.layers['Embedding'].backward(dout)\n",
    "\n",
    "        dhs = dhs0 + dhs1\n",
    "        dh = self.layers['LSTM'].dh + np.sum(dhs, axis=1)\n",
    "        \n",
    "        return dh\n",
    "    \n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.layers['LSTM'].set_state(h)\n",
    "\n",
    "        H = h.shape[1]\n",
    "        peeky_h = h.reshape(1, 1, H)\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape(1,1)\n",
    "            out = self.layers['Embedding'].forward(x)\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            out = self.layers['LSTM'].forward(out)\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            score = self.layers['Affine'].forward(out)\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "\n",
    "        return np.array(sampled)\n",
    "\n",
    "class Seq2SeqNetwork(BaseNetwork):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size, max_grad=5.0):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.max_grad = max_grad\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Encoder'] = Encoder(V, D, H)\n",
    "        self.layers['Decoder'] = PeekyDecoder(V, D, H)\n",
    "        self.lastLayer = TimeSoftmaxWithLoss()\n",
    "        self.adam = AdamContainer(list(self.layers.values()))\n",
    "        \n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.layers['Encoder'].forward(xs)\n",
    "        sampled = self.layers['Decoder'].generate(h, start_id, sample_size)\n",
    "        return sampled\n",
    "        \n",
    "    def loss(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "        h = self.layers['Encoder'].forward(xs)\n",
    "        score = self.layers['Decoder'].forward(decoder_xs, h)\n",
    "        loss = self.lastLayer.forward(score, decoder_ts)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1) #.repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "        \n",
    "        return dhs, da\n",
    "\n",
    "class AttentionWeight(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        hr = h.reshape(N, 1, H) #.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "        \n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "        \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        \n",
    "        return dhs, dh\n",
    "\n",
    "class Attention(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh\n",
    "\n",
    "class TimeAttention(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "        \n",
    "        return dhs_enc, dhs_dec\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        for layer in self.layers.values():\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dhs = layer.backward(dhs)\n",
    "        return dhs\n",
    "\n",
    "class AttentionDecoder(BaseLayer):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_w = (rn(V, D) / 100).astype('f')\n",
    "        lstm_wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        affine_w = (rn(2 * H, V) / np.sqrt(2 * H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Embedding'] = TimeEmbedding(embed_w)\n",
    "        self.layers['LSTM'] = TimeLSTM(lstm_wx, lstm_wh, lstm_b, stateful=True)\n",
    "        self.layers['Attention'] = TimeAttention()\n",
    "        self.layers['Affine'] = TimeAffine(affine_w, affine_b)\n",
    "        \n",
    "        self.params = []\n",
    "        for params in [layer.params for layer in self.layers.values()]:\n",
    "            for param in params:\n",
    "                self.params.append(param)\n",
    "        self.grads = []\n",
    "        for grads in [layer.grads for layer in self.layers.values()]:\n",
    "            for grad in grads:\n",
    "                self.grads.append(grad)\n",
    "        \n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:, -1]\n",
    "        self.layers['LSTM'].set_state(h)\n",
    "        \n",
    "        out = self.layers['Embedding'].forward(xs)\n",
    "        dec_hs = self.layers['LSTM'].forward(out)\n",
    "        c = self.layers['Attention'].forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.layers['Affine'].forward(out)\n",
    "        return score\n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        dout = self.layers['Affine'].backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "        dc, ddec_hs0 = dout[:, :, :H], dout[: ,: ,H:]\n",
    "        denc_hs, ddec_hs1 = self.layers['Attention'].backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.layers['LSTM'].backward(ddec_hs)\n",
    "        denc_hs[:, -1] += self.layers['LSTM'].dh\n",
    "        self.layers['Embedding'].backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:,  -1]\n",
    "        self.layers['LSTM'].set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape(1, 1)\n",
    "            out = self.layers['Embedding'].forward(x)\n",
    "            dec_hs = self.layers['LSTM'].forward(out)\n",
    "            c = self.layers['Attention'].forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.layers['Affine'].forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return np.array(sampled)\n",
    "\n",
    "class AttentionSeq2SeqNetwork(BaseNetwork):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size, max_grad=5.0):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.max_grad = max_grad\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Encoder'] = AttentionEncoder(V, D, H)\n",
    "        self.layers['Decoder'] = AttentionDecoder(V, D, H)\n",
    "        self.lastLayer = TimeSoftmaxWithLoss()\n",
    "        self.adam = AdamContainer(list(self.layers.values()))\n",
    "        \n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.layers['Encoder'].forward(xs)\n",
    "        sampled = self.layers['Decoder'].generate(h, start_id, sample_size)\n",
    "        return sampled\n",
    "        \n",
    "    def loss(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "        h = self.layers['Encoder'].forward(xs)\n",
    "        score = self.layers['Decoder'].forward(decoder_xs, h)\n",
    "        loss = self.lastLayer.forward(score, decoder_ts)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path=None):\n",
    "    if file_path is None or not os.path.exists(file_path):\n",
    "        return ptb.load_data('train')\n",
    "    \n",
    "    words = (open(file_path).read().translate(\n",
    "                str.maketrans({',':'','.':'','-':'',\"'\":'',':':'',';':'','!':'','?':''}))\n",
    "                .lower().strip().split())\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_to_id:\n",
    "            tmp_id = len(word_to_id)\n",
    "            word_to_id[word] = tmp_id\n",
    "            id_to_word[tmp_id] = word\n",
    "            \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "def make_corpus(str):\n",
    "    corpus = []\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in str.lower().strip().split():\n",
    "        if word not in word_to_id:\n",
    "            tmp_id = len(word_to_id)\n",
    "            word_to_id[word] = tmp_id\n",
    "            id_to_word[tmp_id] = word\n",
    "        corpus.append(word_to_id[word])\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y):\n",
    "    x = x / np.sqrt(np.sum(x ** 2))\n",
    "    y = y / np.sqrt(np.sum(y ** 2))\n",
    "    return np.dot(x, y)\n",
    "\n",
    "def most_similar(word_to_id, id_to_word, word_matrix, queries=['you', 'say', 'good'], top=5):\n",
    "    for query in queries:\n",
    "        if query not in word_to_id:\n",
    "            print(f'{query} is not found.')\n",
    "            continue\n",
    "        \n",
    "        print(f'\\n{query}')\n",
    "        query_id = word_to_id[query]\n",
    "        query_vec = word_matrix[query_id]\n",
    "        \n",
    "        vocab_size = len(word_matrix)\n",
    "        similarity = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        \n",
    "        count = 0\n",
    "        for i in (-1 * similarity).argsort():\n",
    "            if id_to_word[i.item()] == query:\n",
    "                continue\n",
    "            print(f' {id_to_word[i.item()]}: {similarity[i.item()]}')\n",
    "            count += 1\n",
    "            if count >= top:\n",
    "                break\n",
    "\n",
    "def generate_seq(network, start_id, sample_size=100):\n",
    "    word_ids = [start_id]\n",
    "    \n",
    "    x = start_id\n",
    "    while len(word_ids) < sample_size:\n",
    "        x = np.array(x).reshape(1,1)\n",
    "        score = network.predict(x)\n",
    "        p = softmax(score.flatten())\n",
    "        x = np.random.choice(len(p), size=1, p=p)\n",
    "        word_ids.append(x.item())\n",
    "        \n",
    "    return word_ids\n",
    "\n",
    "def print_generate_seq(network, word_to_id, id_to_word, start_word='you', load_state=True):\n",
    "    start_id = word_to_id[start_word]\n",
    "    network.reset_rnn_state()\n",
    "    word_ids = generate_seq(network, start_id)\n",
    "    network.reset_rnn_state()\n",
    "    txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "    txt = txt.replace('<eos>', '.\\n')\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question():\n",
    "    num1 = random.randint(0, 999)\n",
    "    num2 = random.randint(0, 999)\n",
    "    question = str(num1) + '+' + str(num2)\n",
    "    correct = '=' + str(num1 + num2)\n",
    "    question = question.ljust(7)\n",
    "    correct = correct.ljust(5+1)\n",
    "    return question, correct\n",
    "\n",
    "def make_question_data(data_size, char_to_id):\n",
    "    questions = []\n",
    "    corrects = []\n",
    "    for _ in range(data_size):\n",
    "        q, c = generate_question()\n",
    "        q = [char_to_id[s] for s in q]\n",
    "        c = [char_to_id[s] for s in c]\n",
    "        questions.append(q)\n",
    "        corrects.append(c)\n",
    "    questions = np.array(questions)\n",
    "    corrects = np.array(corrects)\n",
    "    return questions[:, ::-1], corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_question(max_daydelta=10000):\n",
    "    today = datetime.date.today()\n",
    "    date = today + datetime.timedelta(days=random.randint(-max_daydelta, max_daydelta))\n",
    "    question = date.strftime('%Y %B %d %A')\n",
    "    correct = '=' + date.strftime('%Y-%m-%d')\n",
    "    question = question.ljust(29)\n",
    "    correct = correct.ljust(1+10+1)\n",
    "    return question, correct\n",
    "\n",
    "def make_date_questions(question_num):\n",
    "    questions =  []\n",
    "    corrects = []\n",
    "    char_to_id = {}\n",
    "    \n",
    "    index = 0\n",
    "    for number in range(10):\n",
    "        char_to_id[f'{number}'] = index\n",
    "        index += 1\n",
    "    for alphabet in range(26):\n",
    "        char_to_id[f'{chr(ord(\"a\") + alphabet)}'] = index\n",
    "        char_to_id[f'{chr(ord(\"A\") + alphabet)}'] = index + 1\n",
    "        index += 2\n",
    "    char_to_id['='] = index\n",
    "    char_to_id['-'] = index + 1\n",
    "    char_to_id[' '] = index + 2\n",
    "    \n",
    "    for _ in range(question_num):\n",
    "        q, c = generate_date_question()\n",
    "        q = [char_to_id[s] for s in q]\n",
    "        c = [char_to_id[s] for s in c]\n",
    "        questions.append(q)\n",
    "        corrects.append(c)\n",
    "    questions = np.array(questions)\n",
    "    corrects = np.array(corrects)\n",
    "    \n",
    "    id_to_char = dict(zip(char_to_id.values(), char_to_id.keys()))\n",
    "    \n",
    "    return questions[:, ::-1], corrects, char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Program1:\n",
    "    def __init__(self):\n",
    "        self.params = OrderedDict({'batch_size': 64, 'corpus_size': 202646,\n",
    "                                  'wordvec_size': 512, 'hidden_size': 512,\n",
    "                                  'time_size': 32, 'max_epoch': 100})\n",
    "        self.losses = []\n",
    "        self.ppl_list = []\n",
    "        \n",
    "    def __call__(self):\n",
    "        ## for comet\n",
    "        #experiment = Experiment()\n",
    "        #experiment.log_parameters(self.params)\n",
    "        \n",
    "        corpus, word_to_id, id_to_word = load_data('dataset/tinyshakespare.txt')\n",
    "        #corpus, word_to_id, id_to_word = load_data()\n",
    "        print(f'max_corpus_size: {len(corpus)}')\n",
    "        corpus = corpus[:self.params['corpus_size']]\n",
    "        self.params['corpus_size'] = len(corpus)\n",
    "        vocab_size = int(max(corpus) + 1)\n",
    "        self.params['vocab_size'] = vocab_size\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        \n",
    "        xs = corpus[:-1]\n",
    "        ts = corpus[1:]\n",
    "        data_size = len(xs)\n",
    "        \n",
    "        for key in self.params.keys() :\n",
    "            print(f'{key}: {self.params[key]}')\n",
    "        batch_size, corpus_size, wordvec_size, hidden_size, time_size, max_epoch, vocab_size = self.params.values()\n",
    "\n",
    "        self.net = Network(vocab_size, wordvec_size, hidden_size)\n",
    "        self.net.load()\n",
    "        \n",
    "        print()\n",
    "        print_generate_seq(self.net, word_to_id, id_to_word)\n",
    "        print()\n",
    "        most_similar(word_to_id, id_to_word, self.net.layers['Embedding'].params[0])\n",
    "        print()\n",
    "        \n",
    "        max_iters = data_size // (batch_size * time_size)\n",
    "        time_idx = 0\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "        \n",
    "        jump = (corpus_size - 1) // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]\n",
    "        \n",
    "        for epoch in range(max_epoch):\n",
    "            for ite in tqdm(range(max_iters)):\n",
    "                batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "                batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        \n",
    "                for t in range(time_size):\n",
    "                    for i, offset in enumerate(offsets):\n",
    "                        batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                        batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "                    time_idx += 1\n",
    "                \n",
    "                loss = self.net.train(batch_x, batch_t)\n",
    "                self.losses.append(loss.tolist())\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "                \n",
    "                ## for comet\n",
    "                #experiment.log_metric('loss', loss, step=epoch*max_iters-(max_iters-ite))\n",
    "            \n",
    "            ppl = float(np.exp(total_loss / loss_count))\n",
    "            print(f'| epoch {epoch+1} | total_loss {total_loss} | perplexity {ppl}')\n",
    "            self.ppl_list.append(ppl)\n",
    "            if len(self.ppl_list) > 1 and min(self.ppl_list[:-1]) >= ppl:\n",
    "                self.net.save(f'network.state.ppl_min.pkl')\n",
    "            \n",
    "            time_idx, total_loss, loss_count = 0, 0, 0\n",
    "            \n",
    "            ## for comet\n",
    "            #experiment.log_metric('perplexity', ppl, step=epoch*max_iters)\n",
    "        \n",
    "        print()\n",
    "        print_generate_seq(self.net, word_to_id, id_to_word)\n",
    "        print()\n",
    "        most_similar(word_to_id, id_to_word, self.net.layers['Embedding'].params[0])\n",
    "        \n",
    "        self.net.save()\n",
    "\n",
    "        ## for comet\n",
    "        #experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Program2:\n",
    "    def __init__(self):\n",
    "        self.char_to_id = {'0':0, '1':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9, '+':10, '=':11, ' ':12}\n",
    "        self.id_to_char = dict(zip(self.char_to_id.values(), self.char_to_id.keys()))\n",
    "        self.params = OrderedDict({'vocab_size': len(self.char_to_id), 'wordvec_size': 16, 'hidden_size': 256,\n",
    "                                   'max_grad': 5.0, 'max_epoch': 100, 'sample_size': 50000, 'batch_size': 128})\n",
    "        self.losses = []\n",
    "        \n",
    "    def fit(self, x, t, batch_size):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        \n",
    "        for iters in tqdm(range(max_iters)):\n",
    "            batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "            batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "            loss = self.net.train(batch_x, batch_t)\n",
    "            self.losses.append(loss.tolist())\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "    def __call__(self):\n",
    "        ## for comet\n",
    "        #experiment = Experiment()\n",
    "        #experiment.log_parameters(self.params)\n",
    "        \n",
    "        for key in self.params.keys() :\n",
    "            print(f'{key}: {self.params[key]}')\n",
    "        vocab_size, wordvec_size, hidden_size, max_grad, max_epoch, sample_size, batch_size = self.params.values()\n",
    "\n",
    "        self.net = Seq2SeqNetwork(vocab_size, wordvec_size, hidden_size, max_grad)\n",
    "        self.net.load()\n",
    "        \n",
    "        if sample_size != -1:\n",
    "            questions, corrects = make_question_data(sample_size, self.char_to_id)\n",
    "            \n",
    "        for epoch in range(max_epoch):\n",
    "            if sample_size == -1:\n",
    "                questions, corrects = make_question_data(batch_size, self.char_to_id)\n",
    "            loss = self.fit(questions, corrects, batch_size)\n",
    "            \n",
    "            correct_num = 0\n",
    "            test_num = 3\n",
    "            for _ in range(test_num):\n",
    "                test_q, test_c = make_question_data(1, self.char_to_id)\n",
    "                answer = self.net.generate(test_q, start_id=self.char_to_id['='], sample_size=4)\n",
    "                question = ''.join([self.id_to_char[x] for x in to_cpu(test_q[0, ::-1])])\n",
    "                correct = ''.join([self.id_to_char[x] for x in to_cpu(test_c[0, 1:-1])])\n",
    "                answer = ''.join([self.id_to_char[x] for x in to_cpu(answer)])\n",
    "                is_correct = True if answer == correct else False\n",
    "                correct_num += 1 if is_correct else 0\n",
    "                print(f'{\"○\" if is_correct else \"×\"} {question} = {answer}[{correct}]')\n",
    "            print(f'[{epoch}] Corrects {correct_num} / {test_num}\\tloss: {loss}')\n",
    "            \n",
    "            ## for comet\n",
    "            #experiment.log_metric('loss', loss, step=epoch)\n",
    "        \n",
    "        self.net.save()\n",
    "        self.exam()\n",
    "\n",
    "        ## for comet\n",
    "        #experiment.end()\n",
    "    \n",
    "    def exam(self, question_num=100):\n",
    "        correct_num = 0\n",
    "        questions, corrects = make_question_data(question_num, self.char_to_id)\n",
    "        for i in range(question_num):\n",
    "            answer = self.net.generate(np.expand_dims(questions[i], axis=0), start_id=self.char_to_id['='], sample_size=4)\n",
    "            question = ''.join([self.id_to_char[x] for x in to_cpu(questions[i, ::-1])])\n",
    "            correct = ''.join([self.id_to_char[x] for x in to_cpu(corrects[i, 1:-1])])\n",
    "            answer = ''.join([self.id_to_char[x] for x in to_cpu(answer)])\n",
    "            is_correct = True if answer == correct else False\n",
    "            correct_num += 1 if is_correct else 0\n",
    "            print(f'{\"○\" if is_correct else \"×\"} {question} = {answer}[{correct}]')\n",
    "        print(f'Score: {correct_num} / {question_num}\\tAccuracy: {(correct_num / question_num)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Program3:\n",
    "    def __init__(self):\n",
    "        sample_size = 50000\n",
    "        self.questions, self.corrects, self.char_to_id, self.id_to_char = make_date_questions(sample_size)\n",
    "        self.params = OrderedDict({'vocab_size': len(self.char_to_id), 'wordvec_size': 16, 'hidden_size': 256,\n",
    "                                   'max_grad': 5.0, 'max_epoch': 80, 'sample_size': sample_size, 'batch_size': 128})\n",
    "        self.net = AttentionSeq2SeqNetwork(self.params['vocab_size'], self.params['wordvec_size'], self.params['hidden_size'], self.params['max_grad'])\n",
    "        self.losses = []\n",
    "        \n",
    "    def fit(self, x, t, batch_size):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        \n",
    "        for iters in tqdm(range(max_iters)):\n",
    "            batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "            batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "            loss = self.net.train(batch_x, batch_t)\n",
    "            self.losses.append(loss.tolist())\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def visualize(self, attention_map, row_labels, column_labels):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "        ax.patch.set_facecolor('black')\n",
    "        ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "        ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xticklabels(row_labels, minor=False)\n",
    "        ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "        plt.savefig('figure.png')\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        print('Saved: figure.png')\n",
    "    \n",
    "    def save_figure(self):\n",
    "        global np, device\n",
    "        device_changed = False\n",
    "        \n",
    "        temp_file_name = 'temp.state.pkl'\n",
    "        self.net.save(temp_file_name)\n",
    "        \n",
    "        if device == 'gpu':\n",
    "            print('Change to cpu.')\n",
    "            device = 'cpu'\n",
    "            np = numpy\n",
    "            device_changed = True\n",
    "        \n",
    "        _program = Program3()\n",
    "        _program.net.load(temp_file_name)\n",
    "        \n",
    "        for i in tqdm(range(len(_program.questions))):\n",
    "            question = to_cpu(_program.questions[[i]])\n",
    "            correct = to_cpu(_program.corrects[[i]])\n",
    "            answer = _program.net.generate(question, start_id=self.char_to_id['='], sample_size=10)\n",
    "            _program.net.loss(question, correct)\n",
    "            question = [_program.id_to_char[x] for x in question[0, ::-1]]\n",
    "            correct = [_program.id_to_char[x] for x in correct[0, 1:-1]]\n",
    "            answer = [_program.id_to_char[x] for x in answer]\n",
    "            if answer != correct:\n",
    "                  continue\n",
    "            \n",
    "            attention_map = np.array(_program.net.layers['Decoder'].layers['Attention'].attention_weights)\n",
    "            attention_map = attention_map[:, 0, :].reshape(attention_map.shape[0], attention_map.shape[2])\n",
    "            attention_map = attention_map[:-1, ::-1]\n",
    "            _program.visualize(attention_map, question, correct)\n",
    "            break\n",
    "                  \n",
    "        if device_changed:\n",
    "            print('Change to gpu.')\n",
    "            device = 'gpu'\n",
    "            np = cupy\n",
    "        \n",
    "        os.remove(temp_file_name)\n",
    "        \n",
    "    def __call__(self):\n",
    "        ## for comet\n",
    "        #experiment = Experiment()\n",
    "        #experiment.log_parameters(self.params)\n",
    "        \n",
    "        for key in self.params.keys() :\n",
    "            print(f'{key}: {self.params[key]}')\n",
    "        vocab_size, wordvec_size, hidden_size, max_grad, max_epoch, sample_size, batch_size = self.params.values()\n",
    "\n",
    "        self.net.load()\n",
    "        \n",
    "        for epoch in range(max_epoch):\n",
    "            loss = self.fit(self.questions, self.corrects, batch_size)\n",
    "            \n",
    "            correct_num = 0\n",
    "            test_num = 3\n",
    "            for _ in range(test_num):\n",
    "                questions, corrects, _, _ = make_date_questions(1)\n",
    "                answer = self.net.generate(questions, start_id=self.char_to_id['='], sample_size=10)\n",
    "                question = ''.join([self.id_to_char[x] for x in to_cpu(questions[0, ::-1])])\n",
    "                correct = ''.join([self.id_to_char[x] for x in to_cpu(corrects[0, 1:-1])])\n",
    "                answer = ''.join([self.id_to_char[x] for x in to_cpu(answer)])\n",
    "                is_correct = True if answer == correct else False\n",
    "                correct_num += 1 if is_correct else 0\n",
    "                print(f'{\"○\" if is_correct else \"×\"} {question} = {answer} [{correct}]')\n",
    "            print(f'[{epoch}] Corrects {correct_num} / {test_num}\\tloss: {loss}')\n",
    "            \n",
    "            ## for comet\n",
    "            #experiment.log_metric('loss', loss, step=epoch)\n",
    "        \n",
    "        self.net.save()\n",
    "        self.exam()\n",
    "        self.save_figure()\n",
    "\n",
    "        ## for comet\n",
    "        #experiment.end()\n",
    "    \n",
    "    def exam(self, question_num=100):\n",
    "        correct_num = 0\n",
    "        for _ in range(question_num):\n",
    "            questions, corrects, _, _ = make_date_questions(1)\n",
    "            answer = self.net.generate(questions, start_id=self.char_to_id['='], sample_size=10)\n",
    "            question = ''.join([self.id_to_char[x] for x in to_cpu(questions[0, ::-1])])\n",
    "            correct = ''.join([self.id_to_char[x] for x in to_cpu(corrects[0, 1:-1])])\n",
    "            answer = ''.join([self.id_to_char[x] for x in to_cpu(answer)])\n",
    "            is_correct = True if answer == correct else False\n",
    "            correct_num += 1 if is_correct else 0\n",
    "            print(f'{\"○\" if is_correct else \"×\"} {question} = {answer} [{correct}]')\n",
    "        print(f'Score: {correct_num} / {question_num}\\tAccuracy: {(correct_num / question_num)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = Program1()\n",
    "if __name__ == '__main__':\n",
    "    program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('loss')\n",
    "plt.plot(program.losses, label='loss')\n",
    "plt.legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = Program2()\n",
    "if __name__ == '__main__':\n",
    "    program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('loss')\n",
    "plt.plot(program.losses, label='loss')\n",
    "plt.legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = Program3()\n",
    "if __name__ == '__main__':\n",
    "    program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "plt.plot(program.losses, label='loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "img = Image.open('figure.png')\n",
    "fig = plt.figure(dpi=200)\n",
    "ax = fig.add_subplot(1, 1, 1) # (row, col, num)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
